{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf600
{\fonttbl\f0\froman\fcharset0 TimesNewRomanPSMT;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11960\viewh9680\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\b\fs36 \cf0 Code Archive
\b0\fs28 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b \cf0 \
Testing new Barycentric code\

\b0 April 4, 2019\
I am testing the new Barycentric code written by Shang-Huan Chiu and Bryan. First I am simply testing all the variants of ibary = 1 or 0 and ifmm = 1 or 0 for the simple case of 02circ1-128.thlen. I am making sure that erosion proceeds reasonably well for at least 3-5 output times.\
\
Testing 02circ1-128.thlen\
Parameters			Visual result		cpu time (in minutes) at step 8\
ifmm = 1, ibary = 1	Pass			0.86\
ifmm = 0, ibary = 1	Pass			2.0\
ifmm = 1, ibary = 0	Pass			0.25\
ifmm = 0, ibary = 0	Pass			0.14\
From these tests it is clear that barycentric can slow the simulation. Of course, it also offers higher accuracy for nearly touching bodies presumably. It is a little curious that for ibary=0, the FMM actually slows the simulation. Lets try a test with more bodies and more grid points.\
\
Testing 10circ4-256.thlen\
Parameters			Visual result		cpu time (in minutes) at step 3 & 4\
ifmm = 1, ibary = 1	Pass			13 & 23\
ifmm = 0, ibary = 1	Pass			68 & too long\
ifmm = 1, ibary = 0	Pass			1.3 & 2.3 \
ifmm = 0, ibary = 0	Pass			0.99 & 1.8\
Still, ifmm slows down the simulation in the case of ibary=0. Lets test more bodies.\
\
Testing 30circ2-256.thlen\
Parameters			Visual result		cpu time (in minutes) at step 2 & 3\
ifmm = 1, ibary = 1	-			140 & too long\
ifmm = 0, ibary = 1			\
ifmm = 1, ibary = 0	Pass			5.6 & 9.3\
ifmm = 0, ibary = 0	Pass			7 & 12\
Good, in this test with standard trapezoid rule, the FMM reduces the computational time as it should.\
\

\b Optimal parameters for simulations\

\b0 Feb 19, 2019\
I am planning to run a systematic set of multi body simulations with 10-100 bodies and a few different area fractions. I have already run a few preliminary simulations with 10-30 bodies, using area fraction = 0.5, dtout = 2e-4, and using either 512 or 256 points per body. I foresee a problem with very large folder sizes; for the 30-body case with 512, the folder is about 500 MB (with 150 files and taking 30 hours to run). By using 256 points, I can reduce the folder size (by close to half) and also reduce the run time by about half; also, by making dtout larger I can reduce the file size. Thus, I have decided to use the following default parameters:\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \ul \ulc0 \
Default parameters\ulnone \
npts = 256, epsfac = 15, sigfac = 10, dt = 1e-4, dtout = 3e-4
\b \
\
\
Back to analysis with DataTank\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b0 \cf0 Feb 15, 2019\
I am noticing some obsolete things in the DataTank script and I am deleting some for simplicity. In particular, the Aspect Ratio calculations were for all the bodies total, so it only made sense if there was a single body. Therefore I am deleting this part.
\b \
\

\b0 Feb 18, 2019: 
\b rescaling of drag on umax
\b0 \
I noticed the code is kind of messy and unclear regarding the scaling of drag by umax. In the Julia code, I compute all drag values with umax=1, which means I am computing the drag normalized by umax. Then to calculate the true drag, I multiply by umax. Also, at this second stage, I can rescale umax arbitrarily, and in the DataTank code it looks like I decided to use umax/10 as my new umax (as long as I am consistent with this rescaling). This was a consequence of the choice I made in the Julia code to make umax = 10 * 8/pdrop in the case of fixed pdrop. I don\'92t know exactly why I made this choice, but I guess it was so that I could use comparable values of dt (and maybe other stuff) independent of whether I chose fixpdrop = 0 or 1.\
Also, FYI, the vorticity is calculate in the Julia code as the true vorticity. So if I want rescaled vorticity, I have to rescale by umax.
\b \
\
Revamping codes\

\b0 Feb 12, 2019.\
I made a few more aesthetic changes. I got rid of the annoying extensions in the folder names for the saved figures and data. I changed the folder julia to julia_code. I saved the seed used to generate the initial geometry in the circ file.\
\
Feb 3, 2019.\
I am starting to write the second paper and realized that I had forgotten a lot about how to use these codes. Therefore, I am updating and getting rid of obsolete things to make life easier. These are the updates I made:\
- I removed geos.jl in the main Julia folder. This code was made obsolete by the new codes in make_geos folder. However, the one thing I might want to return to is the routine to make polygon geometries of arbitrary number of sides. That is kind of cool.\
- I removed the entire geometries/ folder since it is obsolete. We are now using initial geometries from geos2/ and I renamed that folder to input_geos/
\b \

\b0 - I changed the folder of output data, called datafiles/, to output_data/; this folder is not on the repo.
\b \

\b0 - I updated Julia codes in make_geos to Julia 1.0\
- I modified the makegeos codes to print a pdf of the geometry in the input_geos folder for quick viewing.\
- I moved the makegeos codes in with the rest of the Julia codes.\
\

\b \
Updated to Julia 1.0\

\b0 January, 2019\
I updated the Julia codes to be compatible with 1.0. Running the erosion simulation now works in Julia 1.0.
\b \
\
New geometry files\

\b0 Summer 2018\
At some point over the summer, I wrote new routines to produce geometries with more control, i.e. one can control the exact volume fraction and size distribution while still using randomized geometries. The geometries are now in a different folder goes2/. In the input file, I simply input the folder and file name.\

\b \
New time-stepping method\

\b0 Feb 18, 2018\
It occurred to me that I could simply use RK2 to update the argument of the integrating factor, int(zeta). Surprisingly, this gives a different integration rule compared to what I had been doing. The advantage is that it probably can generalize to RK4. I ran a convergence test and the new rule is indeed second order.
\b \
\
Comprehensive convergence tests\

\b0 Jan 19, 2018\
I ran thorough convergence tests as dt is refined, using the L2-norm of the shape as my error. I ran an array of cases, starting with a single body shrinking, then keeping the area fixed, then keeping the pressure drop fixed, then running 20 bodies with the pressure drop fixed. All cases showed 2nd order convergence in the body shape as dt is refined. The results are in the file aConvTest.rtf.
\b \
\
Comparing the various 50 body runs regarding resistivity and drag\

\b0 Jan 13, 2018\
The runs to compare are 50circ512p1, p5 (epsfac=30, sigfac=10), and p6 (espfac=25, sigfac=15). There is also 50circ, which only uses 128 points. All look about the same on resistivity: there is some noise, but no major spikes. There are differences in drag though. The run p1 has some huge spikes, which make it unusable; p5 and p6 both are much better. Regarding drag, the quality of the 20 and 40 body runs, as well as 50circ is all about the same as p5 and p6.
\b \
Decision:
\b0  Use run 50circ512p5
\b \
\
Comparing resistivity and drag to runs from Oaxaca talk\

\b0 Jan 12, 2018\
I have changed a lot of things in the code since the Oaxaca talk, and I was worried because the resistivity and drag for the 6-body run looks different now. But there is no need to worry. The biggest change in the code is the ability to fix the pressure drop (instead of the flow rate). The most current 6-body run has a fixed pressure drop. This essentially stretches time when comparing to the plots from my Oaxaca talk. With this in mind, everything looks consistent.\
Note: It is probably most meaningful to compute the drag with umax fixed. When looking at drag with fixed pressure drop, the drag can increase, which is counterintuitive, and I am not sure if it means anything.
\b \
\
Postprocess: separate target point calculations from the rest\

\b0 Dec 28, 2017\
My belief is that the target point stuff is the most expensive, so I separated it from the rest of the post-processing so that I do not have to keep running it. However, running tests on 50circ512 shows that both are really slow. In fact, the regular postprocessing is a bit slower. Nonetheless, separating the two routines will reduce the amount of redundant calculations.\
Jan 12, 2018 Update: I separated postprocessing into 3 steps now.
\b \
\
Big run numerical instability causing crash because umax=inf\

\b0 Around Dec 25, 2017\
I ran a big simulation, 50circ512, with dt = 4e-5. The run crashed near the end because of a numerical instability causing the pressure jump to be zero, which caused umax to be infinity. I could treat this by capping umax, e.g. umax = max(umax, 100). However, this would cause umax to change discontinuously in time, which probably would not be good. Alternatively, I could use the previous value of umax, but this would require nontrivial coding.
\b \
\
Single-body runs with various dt values\

\b0 Dec 12, 2017\
I am running the single-body case with npts = 256, 512, 1024, keeping fixed epsfac = 10, sigfac = 10, and varying dt resolution (with dtout fixed at 2e-4). I am comparing modest resolution dt=2e-4 to ultra-high resolution dt=1e-6. For npts=1024 with dt=2e-4 there are some moderate wiggles on the body right before it vanishes. But with dt=1e-6 there are no wiggles! Also, the plot of area versus time is much smoother with the higher temporal resolution.
\b \
\
New approach: scaling the smoothing on the mean of the absolute shear stress.\

\b0 Dec 1, 2017\
Once I changed to fixing the pressure drop across the cell, it becomes less clear how to scale the curvature-smoothing term because the magnitude of the parabolic flow profile is changing in time. The quickest fix is to include the flow speed, umax, in the scaling. But with multiple bodies, the erosion rate does not depend only on the size of the body and speed of the flow, but also neighboring bodies. It therefore makes more sense to simply scale the smoothing on the average of the absolute shear stress for each individual body. This is the simplest and most direct scaling. The only problem is that matau = mean(abs(tau)) is not known at the next time step, like len is, so the time-stepping has to be modified slightly. This is really no problem though. You can use either the midpoint or the trapezoid rule, depending on which stage of RK2, and work out the formulas. I did this and it is in the methods paper now.\
\
My initial tests with 20 bodies and the new scaling of the smoothing look promising. Also, I ran the convergence test with a single body, and it looks like nearly 2nd order convergence in dt.\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \ul \ulc0 Parameter test\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \ulnone I am testing how small I can make epsfac and sigfac with the new scaling of the smoothing term, using the 20-body geometry.\
GOOD means the simulation runs without errors. BAD means that numerically instabilities develops.\
epsfac = 8, sigfac = 8	GOOD, run A\
epsfac = 10, sigfac = 5	GOOD, run B\
epsfac = 5, sigfac = 20	GOOD, deleted\
epsfac = 5, sigfac = 10	BAD, deleted\
epsfac = 10, sigfac = 1	BAD, deleted\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b \cf0 Conclusion:
\b0  epsfac = 8, sigfac = 8 is a good choice to build from.\
\
Now testing the 6-body geometry\
epsfac = 8, sigfac = 8	BAD\
epsfac = 10, sigfac = 10	BAD\
epsfac = 12, sigfac = 12	NOT GREAT\
epsfac = 15, sigfac = 10	NOT QUITE GOOD\
epsfac = 20, sigfac = 10	GOOD ENOUGH\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \ul Convergence test\ulnone \
Remember to consider there is error in calculating pdrop and umax.\
Test 1: 01circ256.in, espfac = 8, sigfac = 8, tfin = 0.002, fixprdop = 1, nits = 11\
RESULTS\
The dt values: [0.002,0.001,0.0005,0.00025,0.000125,6.2e-5,3.1e-5,1.6e-5,8.0e-6,4.0e-6,2.0e-6]\
The drag values: [9.936,9.898,9.866,9.846,9.835,9.83,9.828,9.827,9.827,9.827,9.827]\
The error: [0.0379,0.0325,0.0197,0.011,0.0051,0.0019,0.0006,0.0002,0.0,0.0]\
The order: [0.22,0.72,0.85,1.1,1.4,1.66,1.83,1.92,1.96]\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b \cf0 Conclusion:
\b0  It looks like 2nd order, though you have to go all the way to dt = 8, 4, or 2 e-6 to see resolution. At dt = 2e-4, the order is close to 1 and the relative error is close to 1e-3.
\b \
\
\
Fix pressure drop\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b0 \cf0 Nov 24, 2017\
First, I wrote code to adjust the maximum of the parabolic profile based on keeping the pressure drop from x = -2 to 2 fixed. There is a corresponding parameter in the params.dat file called fixpdrop. \
Secondly, I adjusted the curvature-smoothing term based on the maximum velocity. In principle, this should make it so that the bodies are not overs-moothed in the beginning when the velocity is slow. I expected this to make a big difference in the body shapes. However, comparing to the plots without this adjustment, there is only a very slight difference. The run I am comparing is 20 bodies.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b \cf0 \
Visualize vorticity with body-fit mesh\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b0 \cf0 Nov 22, 2017\
In postprocess, I wrote code to combine a regular and body-fit mesh to view the vorticity. It works.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b \cf0 \
Runge-Kutta 2 implementation\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b0 \cf0 Nov 7, 2017\
After I determined my RK4 implementation was both unnecessary and flawed, I implemented RK2. The simulation runs and it shows 2nd order convergence!
\b \
\

\b0 Nov 10, 2017\
I had to correct some issues with how I delete bodies when they are too small. There are some tricky issues with the two stages of RK2; i.e. What do you do if the body does not vanish at stage 1, but it does at stage 2? I finally worked all of this out and got a code that seems to work. The best geometry file to test the code is test20.in.\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b \cf0 Convention alpha = s/L or 2 pi s/L
\b0 \
Nov 18, 2017: For the paper, Bryan prefers the 2 pi s /L convention because it really cleans up the Fourier expansion expressions, and I agree. I implemented this change in the paper only early this month. My codes all use the s/L convention. It would be really great, as a check on the formulas, to have my code match the paper, but I think this is asking too much. A bunch of little routines (e.g. even my spectraldiff routine) would have to be modified and I would probably get myself into trouble. So just leave the code as is, with alpha = s/L.\
\

\b List of numerical tricks that do not appear to help at all\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b0 \cf0 1) Krasny filter.\
2) De-aliasing vector products by upsampling.\
3) Giving GMRES a good initial guess.
\b \
\
Ideas from Oaxaca workshop\

\b0 Oct 7, 2017\
I got a few good ideas from the Oaxaca workshop.\
1) It seems we are being very wasteful by computing the shear stress (via the density function) at every single time step. Computing the density function is the most expensive operation and it is changing very little from one time-step to another, so we are spending a lot of effort to essential recompute the same thing from scratch each time. To fix this, we can:\
a) Implement a multi-scale time-stepping procedure, where the stress (and density function) is only computed on large time-steps, and the evolution of theta-L is done with finer time steps. We could linearly extrapolate the stress on the fine-time grid.\
b) We could use the previously computed density function as information for computing the density function at the next time step. Bryan told me previously that GMRES does not benefit immensely from an initial guess, but he expressed some doubt about that later. Alternatively, we could use another iterative method that is more sensitive to the initial guess.\
2) It is attractive to keep the computation in spectral space for as long as possible. I briefly considered simply expanding the Cartesian coordinates xy spectrally and evolving the coefficients in these expansions, so that the method is not necessarily attached to any grid, but rather just a spectral representation. Of course, if we go back to Cartesian representation, we would lose the benefits of the theta-L method. In particular, with theta-L we can add the smoothing curvature-driven term and use implicit (stable) methods to treat this term, and make it area preserving. Going back to Cartesian, the curvature is not easy to compute (nonlinear). We could just ad hod add some diffusion in x-y, but it is not clear how to make that area preserving. Also, when using collocation, we would not stay equally spaced in arc length of course. However, I could remind myself to keep the theta-L computation in spectral space whenever possible. In fact, I believe the code is already implemented like this. We should, however, up-sample when doing products to reduce aliasing errors. This might make the simulation more stable too.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b \cf0 \
Ideas to fix the theta-mean problem\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b0 \cf0 Sep 26, 2017\
I had some ideas to fix the problem regarding the mean of cos, sin theta, but I will probably not implement them because it seems to be a non-issue (see notes above). Nonetheless, the ideas were as follows.\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
For a theta vector describing a closed curve in the equal arc length frame, we have the mean of sin(theta) and cos(theta) is zero. At every step, I check this. When this test fails because either the mean of sin or cos of theta is too big, I need to figure out how to put theta back into the equal arc length frame. I should also smooth theta a bit. Some of my ideas to fix this are as follows.\
1. Use theta to compute x and y, then compute the curvature from x and y (using Eq. 88 in Shelley 1994), then compute a new theta vector from the curvature. The resulting theta should have the mean of sin and cos theta equal to zero. I believe I attempted this strategy in the test file zz.jl, and it seems it did not work.\
2. Set x_alpha = x_alpha/(x_alpha^2 + y_alpha^2) and same for y, then spectrally integrate.
\b \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
Error message in theta check for single-body run
\b0 \
Sep 15, 2017\
In certain situations, we witness an instability in the shape of eroding bodies, eventually resulting in one of my error messages \'93
\b unacceptable theta vector
\b0 \'94. I witnessed this error message for single-body erosion in the case Shrink1024 with dt = 0.02 (but oddly not with the larger dt = 0.05), but did not see evidence of the shape instability. I eventually determined that error message was due to an 
\b imprecise check of the theta end points
\b0 , and once I replaced linear interpolation with quadratic interpolation, I do not get the error message.\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \

\b Simulation results after removing lenevo hack\

\b0 ~March 20, 2017: Due to my convergence test, I removed the lenevo hack. Instead, to keep the body from shrinking, I can subtract the mean-value from atau. This preserves area. After implementing this, the simulations run much better. They show convergence of the Aspect Ratio and the Opening Angle with respect to increasing N and decreasing dt. The opening angle results even seem to agree with my theoretical prediction of 102\'ba now! The simulation is also much more stable now.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \ul \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b \cf0 \ulnone The FMM Works!\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\b0 \cf0 Dec 27, 2016: The FMM is working! Next, Bryan wants to implement a block preconditioner so that multiple bodies runs faster.\
\

\b Mistakes to remember\

\b0 - Problem with deepcopy(): Using deepcopy to copy thlen1 to thlen0 alters thlen0 inside the routine, but for some reason, thlen0 is not modified in the calling routine. Instead I have to use my own copy_thlen!\
- Segmentation faults, malloc_debug errors, and other weird error messages regarding calling the Fortran code. Make sure that the size of the vectors is correct, especially the vectors that are passed in to be modified for output.\
\

\b Fixed parameters in the code\

\b0 In spectral.jl, there is relthold in imagtest() and relthold in krasnyfilter().\
In thetalen.jl, there is thresh in test_theta_means() and test_theta_ends and mthresh in delete_indices()\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardeftab720\pardirnatural\partightenfactor0

\b\fs40 \cf0 \

\fs36 Fortran and Git Notes
\b0\fs28 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b \cf0 Calling Fortran\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b0 \cf0 Good reference: http://julia-programming-language.2336112.n4.nabble.com/example-for-ccall-use-and-fortran-td7737.html\
- You have to mangle the name. Use the command \'91nm\'92 in a terminal to see how the Fortran name is mangled with underscores; e.g. \'91nm libstokes.so\'92\
- Remember, do not include the first leading underscore.\
- If you get weird errors such as segmentation faults or malloc_debug type messages, then one possibility is that you are passing arrays of the wrong size into the Fortran code.\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b \cf0 Compiling Fortran erosion codes\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b0 \cf0 - Run \'93make clean\'94 form fortran_code/ and from fortran_code/fmm.\
- Run \'93make\'94 in fortran_code/fmm, then move files *.so up a directory \'93mv *.so ..\'94\
- Run \'93make\'94 in fortran_code/, then move the files *.so to the julia/ folder.\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b \cf0 Git commands\
\pard\pardeftab720\partightenfactor0

\b0 \cf0 username: nickmoore83, medium password\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardeftab720\pardirnatural\partightenfactor0
\cf0 git clone git@github.com:quaife/erosion.git\
git commit -a -m \'93comment here\'94\
git push\
git pull\
\
git add FileName\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 git ls-files\
\pard\pardeftab720\partightenfactor0
\cf0 git mv\
git rm\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 git rm --cached FileName\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardeftab720\pardirnatural\partightenfactor0
\cf0 - To see which files you changed:\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 git diff --name-only HEAD HEAD~1\
- To see what changes were made:\
\pard\pardeftab720\partightenfactor0
\cf0 git diff HEAD HEAD~3 some_file.jl}